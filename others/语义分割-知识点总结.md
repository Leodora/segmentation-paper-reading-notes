
[TOC]

# 语义分割-知识点总结
目的：汇总一些在语义分割中的知识点，并增加自己的理解

## 1. Depthwise separable convolution

### 1.1 参考文献：  
[1]	 [深度學習-MobileNet (Depthwise separable convolution)](https://medium.com/@chih.sheng.huang821/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-mobilenet-depthwise-separable-convolution-f1ed016b3467)

### 1.2 正文
1.	MobileNet，由Google在2017年提出。主要是为了减少模型的参数量和计算量，同时保持模型的性能。
2.	在参数众多的网络模型中，计算量最多的运算发生在卷积运算中（全连接和卷积不同，它比卷积的运算量更大，可以使用SVD分解拆分全连接层减少计算量）。这里把卷积运算拆分成了两部分:
	+ Depthwise convolution：针对输入资料的每一个Channel都建立一个k*k的Kernel，然后每一个Channel针对对应的Kernel都各自(分开)做convolution。这步骤和一般卷积不太一样，一般的卷积计算是每个Kernel Map都要和所有channel都去做convolution，这边是分开独立去做。各个卷积核只负责一个对应的通道，不会去和别的通道发生卷积行为。
	+ Pointwise convolution：在输入资料的每个channel做完depthwise convolution后，针对每个点，跨越所有channel做pointwise convolution。实际做法是建立Nk个1\*1\*Nch的kernel Map，将depthwise convolution的输出做一般1*1的卷积计算

### 示意图

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/seperable-convolution.png" width="60%" height="60%" />
</div>


## 2. atrous convolution 带洞卷积

### 2.1 参考文献：  
[1]	[https://www.zhihu.com/question/49630217](https://www.zhihu.com/question/49630217)

### 2.2 示意图：  

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/atrous-convolution-1.jpg" width="60%" height="60%" />
</div>


+ （a）是正常的卷积，最下面是输入特征，最上面是输出特征，中间是卷积。卷积就是那三条线。
+ （b）是带洞卷积，中间三条红色的线就是卷积核大小为3时的卷积过程，可以看到是从5个点找了依次相隔一个的点，也就是3个点来进行卷积。相邻点之间的距离dis为1，这里的rate=2表示要有两个dis，说明有三个相邻点，才能有两个dis。 
如果rate = 3，卷积核仍为3，则应该每次间隔2个点进行卷积

### 2.3 总结：  
1. 是输入的值“空洞”了，而非卷积核“空洞”了。空洞指的是跳过固定多的值
2. rate=n，则应该每间隔n-1个点进行一次卷积
3.	Note that standard convolution is a special case in which rate r = 1

## 3. Spatial pyramid pooling

### 3.1 参考文献：  
+ [1]	[https://www.jianshu.com/p/e36aef9b7a8a](https://www.jianshu.com/p/e36aef9b7a8a)  
+ [2]	[https://www.jianshu.com/p/884c2828cd8e](https://www.jianshu.com/p/884c2828cd8e)  
+ [3]	[http://www.cnblogs.com/marsggbo/p/8572846.html#commentform](http://www.cnblogs.com/marsggbo/p/8572846.html#commentform)  
+ [4]	[https://github.com/yueruchen/sppnet-pytorch/blob/master/spp_layer.py](https://github.com/yueruchen/sppnet-pytorch/blob/master/spp_layer.py)


**论文地址**：[Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729)

SPP提出的初衷是为了解决CNN对输入图片尺寸的限制。由于全连接层的存在，与之相连的最后一个卷积层的输出特征需要固定尺寸，从而要求输入图片尺寸也要固定

### 3.2 前言
#### 解决方案：
+ 方案1：对输入进行resize，统一到同一大小。
+ 方案2：取消全连接层，对最后的卷积层global average polling（GAP。
+ 方案3：在第一个全连接层前，加入SPP layer。本文要介绍的。

方案1，即spp-net之前的做法是将图片裁剪或变形（crop/warp），如下图所示

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/Spatial-pyramid-pooling-1.jpg" width="60%" height="60%" />
</div>

但是crop/warp的一个问题是导致图片的信息缺失或变形，影响识别精度。对此，文章中在最后一层卷积特征图的基础上又进一步进行处理，提出了spatial pyramid pooling，即方案3，如图所示：

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/Spatial-pyramid-pooling-2.jpg" width="60%" height="60%" />
</div>

以VGG16网络为例，如下图

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/Spatial-pyramid-pooling-3.jpg" width="60%" height="60%" />
</div>

现有两种规格输入：224\*224\*3和180\*180\*3  
准备在全连接层前加入spp net，也就是上图7\*7\*512那一层后。  

+ 224\*224\*3：全连接层前卷积层大小7\*7\*512
+ 180\*224\*3：全连接层前卷积层大小5\*5\*517  

由于这样不同大小卷积层全连接到1\*1\*4096，权值W是不一样的，所以要统一全连接的输入大小

### 3.3 SPP layer方法：
用不同size，stride的pooling layer，对全连接层前的卷积层进行pooling，然后做flatten。见下图

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/Spatial-pyramid-pooling-4.jpg" width="60%" height="60%" />
</div>

对于一个三层的金字塔（也可以设置为多层的金字塔，如作者在detection问题中使用过a 4-level spatial pyramid (1x1, 2x2, 3x3, 6x6, totally 50 bins)），将任意尺寸的feature map分别切分成16、4、1份（这里16、4、1即为各个层bin的数量），再对每一份进行池化操作（一般选择max pooling），将池化后的结果拼接得到固定长度的特征向量（图中的256为filter的个数），送入全连接层进行后续操作。

原文给出的计算公式如下[3]：

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/Spatial-pyramid-pooling-5.png" width="50%" height="50%" />
</div>

**按照[1]中的例子进行计算：**  
输入512\*7\*7时：bin大小分别为4\*4, 2\*2, 1\*1。  
对于第一个pooling layer(bin_size = 4*4): 此时  

+ kernel_size = ceil(7 / 4, 7 / 4) = (2, 2)
+ stride = floor(7 / 4, 7 / 4) = (1, 1)   

按照Output = (W-Kernel+2P)/S+1 （其中P是padding）的计算公式，这里令Padding = 0，则Output = （7 - 2 + 2*0）/1 + 1 = 6， 即pooling后的输出feature大小为6 x 6。

**而目标输出应该是4 x 4，即bin的大小。**

这里是公式上出现了问题，原文给的公式有两个问题： 

1. 没有给出padding的计算方法（虽然很多文章都没有给，但是在这里padding是比较重要的）
2.	stride应该是向上取整（ceil）而非向下取整


按照原文的公式来计算，即使是不加padding，也会因为stride太小而导致输出的特征图太大。

**基于以上两点原因，结合[4]中的代码实现，公式应该更正为（h和w计算方法相同）**
 <div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/Spatial-pyramid-pooling-6.png" width="40%" height="40%" />
</div>

output_size = floor((H-K+2*P)/S+1) (H为输入特征高度)

假设h=w，图片的长和宽是相等的，则

+ 上图中K表示kernel_size，即下面的size，也就是pooling的滑动窗口大小。
+ S表示stride。
+ p表示padding。
+ h_new和w_new表示加上padding后的高和宽
+ n表示bin的大小。bin在这里指的是要输出的格子数量。比如下面的公式的bin分别为4x4, 2x2, 1x1

**按照修正后的公式举例：**
**输入7\*7\*512时：**  
pooling layer1(输出4\*4\*512): 

+ kernel_size = 7/4(向上取整)=2, 
+ stride = 7/4(向上取整)=2, 
+ padding = floor((2*4-7+1)/2) = 1, 
+ output = floor[(7-2+2*1)/2+1] = floor(4.5) = 4

pooling layer2(输出2\*2\*512): 

+ kernel_size = 7/2(向上取整)=4, 
+ stride = 7/2(向上取整)=4, 
+ padding = f((4*2-7+1)/2)=1, 
+ output = f[(7-4+2*1)/4+1] = f(2.5) = 2

pooling layer3(输出1\*1\*512):  
+ kernel_size = 7/1(向上取整)=7, 
+ stride = 7/1(向上取整)=7, 
+ padding = f((7*1-7+1)/2)=0, 
+ output = f[(7-7+0)/7+1] = 1 

然后做flatten，输出（4\*4+2\*2+1）\*512 = 21\*512，即把一个有512个通道的21维向量送到接下来的全连接层


**输入5\*5\*512时：**  
pooling layer1(输出4\*4\*512): 

+ kernel_size = 5/4(向上取整)=2, 
+ stride = 5/4(向上取整)=2, 
+ padding = f((2*4-5+1)/2) = f(2) = 2, 
+ output = f[(5-2+2*2)/2+1] = f(4.5) = 4 


pooling layer2(输出2\*2\*512): 

+ kernel_size = 5/2(向上取整)=3, 
+ stride = 5/2(向上取整)=3, 
+ padding = f((3*2-5+1)/2) = f(1) = 1, 
+ output = f[(5-3+2*1)/3+1] = f(7/3) = 2 

pooling layer3(输出1\*1\*512):  

+ kernel_size = 5/1(向上取整)=5, 
+ stride = 5/1(向上取整)=5, 
+ padding = f((5*1-5+1)/2) = f(0.5) = 0, 
+ output = f[(5-5+0)/5+1] = f(1) = 1 

然后做flatten，输出（4\*4+2\*2+1）\*512=21\*512

这样全连接层输入都是21*512，是跟网络输入图像size大小无关的。

### 3.4 总结：为什么SPP有优势  
1.	传统的Pooling无法统一尺寸，这里对不同尺寸的feature给不同的size和stride的pooling。其实在这一步已经解决了输入fc层尺寸不统一的问题。
2.	为了减少pooling层对信息的损失，尽可能的保留更丰富的特征信息，要经过多个尺寸的pooling layer，并把三者的结果连接在一起以使得FC层的输入信息更加丰富。即 

## 4. Xception model

### 4.1 参考文献
+ [1] Xception论文原文：[Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)
+ [2] [Xception算法详解](https://blog.csdn.net/u014380165/article/details/75142710)
+ [3] [精读深度学习论文(10) Xception](https://zhuanlan.zhihu.com/p/33778384)


**一句话总结Xception：**
In short, the Xception architecture is a linear stack of depthwise separable convolution layers with residual connections.  

Xception = linear stack ( depthwise separable convolution + residual connections)

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/Xception-net-structure.jpg" width="80%" height="80%" />
</div>

### 4.2 介绍

Xception是google继Inception后提出的对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。

#### The Inception Module

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/Inception-module.png" width="120%" height="120%" />
</div>


> 当时提出Inception的初衷可以认为是：特征的提取和传递可以通过1\*1卷积，3\*3卷积，5\*5卷积，pooling等，到底哪种才是最好的提取特征方式呢？  
> Inception结构将这个疑问留给网络自己训练，也就是将一个输入同时输给这几种提取特征方式，然后做concat。  


#### Inception V3

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/InceptionV3-1.png" width="60%" height="60%" />
</div>

上图为原始的Inception V3结构。  
> Inception v3和Inception v1（googleNet）对比主要是将5\*5卷积换成两个3\*3卷积层的叠加。

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/InceptionV3-2.png" width="60%" height="60%" />
</div>

上图为简化的Inception结构，去除Inception V3中的avg pool，这样输入的一步操作就都是1*1卷积。

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/InceptionV3-3.png" width="60%" height="60%" />
</div>

再次延伸，提取1*1卷积的公共部分

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/InceptionV3-4.png" width="60%" height="60%" />
</div>

再推广，“极限情况”：先进行普通卷积操作，再对1\*1卷积后的每个channel分别进行3\*3卷积操作，最后将结果concat.

#### Xception
Xception其实就是对depthwise separable convolution和res connection的多次重复使用。

### 4.3 Depthwise Separable Convolution 与 "extreme" version of Inception Module比较 (参考[3])

+ "extreme" version of Inception Module：具体操作过程可以参考上图。
	+ 第一步：普通1\*1卷积。
	+ 第二步：对1\*1卷积结果的每个channel，分别进行3*3卷积操作，并将结果concat。
+ Depthwise Separable Convolution的结构在MobileNet V1中有详细介绍。
	+ 第一步：depthwise卷积，对输入的每个channel，分别进行3\*3卷积操作，并将结果concat。
	+ 第二步：pointwise卷积，对depthwise卷积中的concat结果，进行1\*1卷积操作。
+ Depthwise Separable Convolution 与 "extreme" version of Inception Module的区别：
	+ 操作循序不一致：Depthwise Separable Convolution先进行3\*3卷积，再进行1\*1卷积；Inception先进行1\*1卷积，再进行3\*3卷积。
	+ 是否使用非线性激活操作：Inception中，两次卷积后都使用Relu；Depthwise Separable Convolution中，在depthwise卷积后一般不添加Relu。在本论文中，通过试验进行验证。具体原因在论文MobileNet V2中有解释。

## 其他零碎的知识点
1. 大多数以FCNs为基础的语义分割网络（如deeplab v3），都是在前面有一个特征提取模块作为**backbone**。如resnet-101这样的网络，去掉其FC层，把最后得到的特征用于语义分割。
2. dense pixel prediction即为semantic segmentation

## 致谢
本文使用了非常多的参考文献中的知识，在这里向所有对本文提供了有帮助内容的作者表示感谢。

**本文仅作为学术交流使用。**