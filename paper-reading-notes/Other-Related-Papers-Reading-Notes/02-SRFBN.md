### The progress of the reading plan: 
| Index  |  In Other | All |
| :----: | :-------: | :-------: |
| 20 | 02 | 50 |

## Paper Information
#### Paper Title : 
[Feedback Network for Image Super-Resolution](https://arxiv.org/pdf/1903.09814) 

#### Conference : 
CVPR 19

#### Authors and Institutions
##### Authors
+ Zhen Li 1 
+ Jinglei Yang 2 
+ Zheng Liu 3 
+ Xiaomin Yang 1 
+ Gwanggil Jeon 4 
+ Wei Wu 1


##### Institutions
+ 1 Sichuan University, 
+ 2 University of California, Santa Barbara, 
+ 3 University of British Columbia,+ 4 Incheon National University


#### Official Codes
[https://github.com/Paper99/SRFBN_CVPR19](https://github.com/Paper99/SRFBN_CVPR19)

#### Some articles to comprehend this paper


#### Network Structure

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/03.png" width="80%" />
</div>

## Note
In a word:
"(Maybe the first) Introduce the skip connection into RNN."

+ LR: low resolution image 
+ HR: GT high resolution image
+ SR: high resolution image generated by super-resolution algorithm

### Introduction
Use a RNN like structure called feedback network.

The recurrent structure is often employed to reduce the parameter storage problem.

And the difference between SRFBN and RNN is that the input of SRFBN is always LR, but for RNN, the input of the next iteration is always the output of the last iteration.

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/01.png" width="60%" />
</div>

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/02.png" width="80%" />
</div>

### Feedforward vs. Feedback
#### Feedforward manner:
Most of the not RNN networks are only feedforward.

#### Feedback manner:
For feedforward manner, it's impossible for previous layers to access useful information from the following layers.

So this paper is adopting the feedback manner.

The feedback mechanism in these architectures works in a top-down manner, carrying high-level information back to previous layers and refining low-level encoded information.

The input of SRFBN is always LR. And high-level feature from the last iteration is merged.

### Feedback block
<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/04.png" width="80%" />
</div>

There are G groups of upsample and downsample sets (deconv and conv) with skip connection. This can generate powerful high-level representations.

### curriculum-based training strategy
In each iteration of training, different solution of HR is employed as the training target.

### auxiliary loss
tying the loss at each iteration (to force the network to reconstruct an SR image at each iteration and thus allow the hidden state to carry a notion of high-level information)

### Key Words
Super Resolution; Feedback


## Five questions about this paper:

### 1. [Problem Definition / Motivation] What problem is this paper trying to solve? 
The existing methods don't have the ability of using high-level information to refine low level representations. Only feedforward is not enough.

### 2. [Contribution / Method] What's new in this paper? / How does this paper solve the above problems?
Proposing a feedback network with curriculum-based training strategy to use high-level information back to previous layers and refining low-level encoded information.

### 3. Details about the experiment

#### 3.1 Which Datasets are used?
For training: 

+ DIV2K
+ Flickr2K

Benchmark:

+ Set5
+ Set14
+ B100
+ Urban100
+ Manga109


#### 3.2 How is the experiment set up?
\

#### 3.3 What's the evaluation metric?

+ PSNR
+ SSIM

#### 3.4 Ablation Study

##### 3.4.1 Study of T and G
T is iterations of the RNN structure.
G is the number of groups in the feedback block.

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/05.png" width="80%" />
</div>

As we can see, the more the better.

And finally T=4 and G=6 is used.

But why not keep trying the bigger T and G?

##### 3.4.2 Feedback vs. feedforward
<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/06.png" width="80%" />
</div>

#### 3.5 What is the ranking of the experiment results?
<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/08.png" width="80%" />
</div>

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/10.png" width="80%" />
</div>

State-of-the-art

Here's some comparison:
<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/09.png" width="80%" />
</div>

<div  align="center">    
<img src="https://raw.githubusercontent.com/zhixuanli/segmentation-paper-reading-notes/master/images-folder/20-SRFBN/12.png" width="80%" />
</div>


### 4. Advantages (self-summary rather than the author's)

Feedback manner design can improve representative ability of the low-level layers and make them more corret.

### 5. Disadvantages (self-summary rather than the author's)